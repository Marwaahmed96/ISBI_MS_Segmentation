{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1493b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense, Conv3D, Dropout, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Conv3DTranspose, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from config import *\n",
    "# force channels-last ordering\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "print(tf.keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "452d2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eagerly=False\n",
    "fold=0\n",
    "class cascade_model:\n",
    "    def __init__(self, options):\n",
    "        self.channels = len(options['modalities'])\n",
    "        self.train_split_perc = options['train_split']\n",
    "        self.num_epochs = options['max_epochs']\n",
    "        self.max_epochs_patience = options['patience']\n",
    "        self.verbose=options['net_verbose']\n",
    "        # save model to disk to re-use it. Create an experiment folder\n",
    "        # organize experiment\n",
    "        self.nets_path=os.path.join(options['weight_paths'], options['experiment'], 'nets')\n",
    "        self.Path(nets_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.padding='same'\n",
    "        self.strides=(1,1,1)\n",
    "        self.shape=(*options['patch_size'],channels)\n",
    "        self.pooling_kernel=(2,2,2)\n",
    "        self.pooling_strides=(2,2,2)\n",
    "        self.drop_rate=0.5\n",
    "        self.objective_loss_function=tf.keras.losses.CategoricalCrossentropy\n",
    "        self.load_weights=options['load_weights']\n",
    "        \n",
    " \n",
    "    def network_layers(self,n=\"1\"):\n",
    "        input_layer=Input(shape=self.shape, name='in'+n)\n",
    "\n",
    "        conv1= Conv3D(32, 3, padding=self.padding, strides=self.strides, name='conv1_'+n)(input_layer)\n",
    "        batch_norm1=BatchNormalization(axis=-1,name = 'BN1'+n)(conv1)\n",
    "        pool1= MaxPooling3D(pool_size=self.pooling_kernel,strides=self.pooling_strides, name='avgpool1_'+n)(batch_norm1)\n",
    "\n",
    "        conv2= Conv3D(64, 3, padding=self.padding, strides=self.strides, name='conv2_'+n)(pool1)\n",
    "        batch_norm2=BatchNormalization(axis=-1,name = 'BN2_'+n)(conv2)\n",
    "        pool2= MaxPooling3D(pool_size=self.pooling_kernel,strides=self.pooling_strides, name='avgpool2_'+n)(batch_norm2)\n",
    "\n",
    "        dr = Dropout(name = 'l2drop', rate=self.drop_rate)(pool2)\n",
    "        dens1 = Dense( name='d1_'+n, units = 256)(dr)\n",
    "        dens2 = Dense( name = 'out', units = 2)(dens1)\n",
    "        act=Activation('softmax')(dens2)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=act)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def get_callbacks(self,weights_file_path, fold, initial_learning_rate=0.0001, learning_rate_drop=0.5,\n",
    "                      learning_rate_patience=50, verbosity=1, early_stopping_patience=None):\n",
    "\n",
    "        check_point = ModelCheckpoint(weights_file_path+'fold_' + fold + '_weights-{epoch:02d}-{val_loss:.2f}.hdf5', save_best_only=True)\n",
    "        csv_log = CSVLogger(weights_file_path+'training-log.csv', append=True)\n",
    "\n",
    "        # potential problem of recude learning rate: https://github.com/keras-team/keras/issues/10924\n",
    "        reduce = ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience, verbose=verbosity)\n",
    "        if early_stopping_patience:\n",
    "            early_stop = EarlyStopping(verbose=verbosity, patience=early_stopping_patience)\n",
    "            return [check_point, csv_log, reduce, early_stop]\n",
    "        else:\n",
    "            return [check_point, csv_log, reduce]\n",
    "    \n",
    "    def get_cascade_model(self):\n",
    "        \n",
    "        # --------------------------------------------------\n",
    "        # first model\n",
    "        # --------------------------------------------------\n",
    "        net_model = 'model_1'\n",
    "        net1=self.network_layers(n=\"1\")\n",
    "        \n",
    "        net1.compile(optimizer=Adam(lr=0.001), \n",
    "                      loss=self.objective_loss_function, metrics='mse',run_eagerly=run_eagerly)\n",
    "        \n",
    "        path_w1=os.path.join(self.nets_path,  net_model)\n",
    "        callbacks = model.get_callbacks(path_w1, str(fold),\n",
    "                                initial_learning_rate=0.001,\n",
    "                                learning_rate_drop=0.1,\n",
    "                                learning_rate_patience=self.max_epochs_patience,\n",
    "                                early_stopping_patience=self.max_epochs_patience)\n",
    "\n",
    "        \n",
    "        #batch_iterator_train=Rotate_batch_Iterator(batch_size=128)\n",
    "        history=model.fit(x=train_generator,\n",
    "                  batch_size=config.batch_size,\n",
    "                  epochs=self.num_epochs,\n",
    "                  verbose=self.verbose,\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=valid_generator,\n",
    "                  validation_batch_size=128,\n",
    "                  workers=4)\n",
    "        \n",
    "\n",
    "        net_model = 'model_2'\n",
    "        net2=self.network_layers(n=\"2\")\n",
    "        net2.compile(optimizer=Adam(lr=0.001), \n",
    "              loss=self.objective_loss_function, metrics='mse',run_eagerly=run_eagerly)\n",
    "        \n",
    "        path_w2=os.path.join(self.nets_path,  net_model)\n",
    "        callbacks = model.get_callbacks(path_w2, str(fold),\n",
    "                                initial_learning_rate=0.001,\n",
    "                                learning_rate_drop=0.1,\n",
    "                                learning_rate_patience=self.max_epochs_patience,\n",
    "                                early_stopping_patience=self.max_epochs_patience)\n",
    "\n",
    "            # upload weights if set\n",
    "        if self.load_weights == 'True':\n",
    "            print (\"    --> CNN, loading weights from\", options['experiment'], 'configuration')\n",
    "            net1.build((None ,*self.shape))\n",
    "            net1.load_weights(path_w1)\n",
    "        \n",
    "            net2.build((None ,*self.shape))\n",
    "            net2.load_weights(path_w2)\n",
    "        return [net1, net2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
